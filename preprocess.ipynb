{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78446825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK: D:\\Infosys_AI-Tracefinder\\Notebooks\n",
      "OFFICIAL_DIR exists: True\n",
      "FLATFIELD_DIR exists: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — CONFIG (edit these before running)\n",
    "import os\n",
    "WORK = r\"D:\\Infosys_AI-Tracefinder\\Notebooks\"   # <<-- Set this to your project working dir (where outputs will be written)\n",
    "OFFICIAL_DIR = r\"D:\\Infosys_AI-Tracefinder\\Data\\Official\"   # <<-- point to the Official dataset root (scanner folders)\n",
    "FLATFIELD_DIR = r\"D:\\Infosys_AI-Tracefinder\\Data\\Flatfield\" # <<-- flatfield images folder (optional but recommended)\n",
    "OUTPUT_DIR = WORK   # where .pkl/.npy files will be saved\n",
    "IMG_SIZE = (256, 256)\n",
    "DENOISE_METHOD = \"wavelet\"   # keep \"wavelet\" (mentor's method); \"wiener\" optional\n",
    "VALID_EXTS = ('.tif', '.tiff', '.png', '.jpg', '.jpeg')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(\"WORK:\", WORK)\n",
    "print(\"OFFICIAL_DIR exists:\", os.path.exists(OFFICIAL_DIR))\n",
    "print(\"FLATFIELD_DIR exists:\", os.path.exists(FLATFIELD_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e57754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Imports & utilities\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pywt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from scipy.signal import wiener as scipy_wiener\n",
    "\n",
    "def safe_makedirs(p): \n",
    "    os.makedirs(p, exist_ok=True)\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(\"Saved:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "647ec860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Denoising & preprocessing \n",
    "def to_gray(img):\n",
    "    if img is None:\n",
    "        return None\n",
    "    if img.ndim == 3:\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "\n",
    "def resize_to(img, size=IMG_SIZE):\n",
    "    return cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def normalize_img(img):\n",
    "    # map to float32 [0,1] — consistent with mentor notebook\n",
    "    return img.astype(np.float32) / 255.0\n",
    "\n",
    "def denoise_wavelet_safe(img, wavelet='db4', level=2):\n",
    "    \"\"\"\n",
    "    Soft-threshold wavelet denoising using robust MAD estimate.\n",
    "    Returns denoised float32 image same shape as input.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(img, dtype=np.float32)\n",
    "    try:\n",
    "        coeffs = pywt.wavedec2(arr, wavelet=wavelet, level=level)\n",
    "    except Exception as e:\n",
    "        # fallback: return original if decomposition fails\n",
    "        return arr.copy()\n",
    "    # collect detail coeffs\n",
    "    details = []\n",
    "    for d in coeffs[1:]:\n",
    "        for comp in d:\n",
    "            details.append(np.ravel(np.nan_to_num(comp)))\n",
    "    if len(details) == 0:\n",
    "        return arr.copy()\n",
    "    details = np.concatenate(details)\n",
    "    mad = np.median(np.abs(details - np.median(details)))\n",
    "    sigma = mad / 0.6745 if mad > 0 else 0.0\n",
    "    uthresh = sigma * np.sqrt(2 * np.log(arr.size + 1e-12))\n",
    "    # threshold details\n",
    "    new_coeffs = [coeffs[0]]\n",
    "    for d in coeffs[1:]:\n",
    "        new_level = tuple(pywt.threshold(np.nan_to_num(comp), value=uthresh, mode='soft') for comp in d)\n",
    "        new_coeffs.append(new_level)\n",
    "    den = pywt.waverec2(new_coeffs, wavelet)\n",
    "    den = den[:arr.shape[0], :arr.shape[1]]\n",
    "    return np.nan_to_num(den, nan=arr).astype(np.float32)\n",
    "\n",
    "def gaussian_highpass(img, ksize=11):\n",
    "    arr = np.asarray(img, dtype=np.float32)\n",
    "    if ksize % 2 == 0:\n",
    "        ksize += 1\n",
    "    blur = cv2.GaussianBlur(arr, (ksize, ksize), 0)\n",
    "    return (arr - blur).astype(np.float32)\n",
    "\n",
    "def denoise_mentor(img, method=DENOISE_METHOD):\n",
    "    \"\"\"\n",
    "    Mentor-style denoise: default wavelet (db4 L2). \n",
    "    Returns denoised image float32.\n",
    "    \"\"\"\n",
    "    if method == \"wiener\":\n",
    "        return scipy_wiener(img, mysize=(5,5)).astype(np.float32)\n",
    "    # default wavelet fallback\n",
    "    return denoise_wavelet_safe(img, wavelet='db4', level=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d1361ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — preprocess single file -> residual\n",
    "def preprocess_image_path(path, method=DENOISE_METHOD, size=IMG_SIZE):\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = to_gray(img)\n",
    "    img = resize_to(img, size=size)\n",
    "    img = normalize_img(img)\n",
    "    den = denoise_mentor(img, method=method)\n",
    "    # residual used by mentor: img - den\n",
    "    residual = (img - den).astype(np.float32)\n",
    "    return residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1fae571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Official: 100%|██████████| 11/11 [02:51<00:00, 15.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Infosys_AI-Tracefinder\\Notebooks\\official_residuals.pkl\n",
      "Scanners in official_residuals: ['Canon120-1', 'Canon120-2', 'Canon220', 'Canon9000-1', 'Canon9000-2', 'EpsonV370-1', 'EpsonV370-2', 'EpsonV39-1', 'EpsonV39-2', 'EpsonV550']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Flatfield: 100%|██████████| 11/11 [00:01<00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Infosys_AI-Tracefinder\\Notebooks\\flatfield_residuals.pkl\n",
      "Scanners in flatfield_residuals: ['Canon120-1', 'Canon120-2', 'Canon220', 'Canon9000-1', 'Canon9000-2', 'EpsonV370-1', 'EpsonV370-2', 'EpsonV39-1', 'EpsonV39-2', 'EpsonV550']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — process dataset directory and save official/flatfield residual dict\n",
    "def process_dataset_dir(base_dir, save_path=None, expect_dpi=True):\n",
    "    \"\"\"\n",
    "    Walks base_dir scanner folders and returns dict {scanner: [residuals...]}\n",
    "    Handles DPI subfolders and images directly in scanner folder.\n",
    "    If save_path is provided, saves the dict to that path.\n",
    "    \"\"\"\n",
    "    residuals = {}\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"⚠ Dataset not found: {base_dir}\")\n",
    "        return residuals\n",
    "    scanners = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
    "    for scanner in tqdm(scanners, desc=f\"Processing {os.path.basename(base_dir)}\"):\n",
    "        scanner_dir = os.path.join(base_dir, scanner)\n",
    "        collected = []\n",
    "        # find dpi subfolders\n",
    "        subdirs = [os.path.join(scanner_dir, s) for s in os.listdir(scanner_dir) if os.path.isdir(os.path.join(scanner_dir, s))]\n",
    "        if subdirs:\n",
    "            # prefer scanning DPI subfolders (150,300 etc.)\n",
    "            for sdir in subdirs:\n",
    "                files = [os.path.join(sdir, f) for f in os.listdir(sdir) if f.lower().endswith(VALID_EXTS)]\n",
    "                for fp in files:\n",
    "                    res = preprocess_image_path(fp)\n",
    "                    if res is not None:\n",
    "                        collected.append(res)\n",
    "        else:\n",
    "            # images directly inside scanner folder\n",
    "            files = [os.path.join(scanner_dir, f) for f in os.listdir(scanner_dir) if f.lower().endswith(VALID_EXTS)]\n",
    "            for fp in files:\n",
    "                res = preprocess_image_path(fp)\n",
    "                if res is not None:\n",
    "                    collected.append(res)\n",
    "        residuals[scanner] = collected\n",
    "    if save_path:\n",
    "        save_pickle(residuals, save_path)\n",
    "    return residuals\n",
    "\n",
    "# Run for Official dataset\n",
    "OFFICIAL_OUT = os.path.join(OUTPUT_DIR, \"official_residuals.pkl\")\n",
    "official_residuals = process_dataset_dir(OFFICIAL_DIR, save_path=OFFICIAL_OUT)\n",
    "print(\"Scanners in official_residuals:\", list(official_residuals.keys())[:10])\n",
    "\n",
    "# Optionally run for flatfield (if you have flatfield images directory)\n",
    "if os.path.exists(FLATFIELD_DIR):\n",
    "    FLATFIELD_OUT = os.path.join(OUTPUT_DIR, \"flatfield_residuals.pkl\")\n",
    "    flatfield_residuals = process_dataset_dir(FLATFIELD_DIR, save_path=FLATFIELD_OUT)\n",
    "    print(\"Scanners in flatfield_residuals:\", list(flatfield_residuals.keys())[:10])\n",
    "else:\n",
    "    print(\"Flatfield dir not found — skipping flatfield generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cd06181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fingerprints: 100%|██████████| 11/11 [00:00<00:00, 3550.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Infosys_AI-Tracefinder\\Notebooks\\scanner_fingerprints.pkl\n",
      "Saved fingerprints: D:\\Infosys_AI-Tracefinder\\Notebooks\\scanner_fingerprints.pkl keys: D:\\Infosys_AI-Tracefinder\\Notebooks\\fp_keys.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — compute scanner_fingerprints.pkl from flatfield_residuals.pkl\n",
    "FP_OUT = os.path.join(OUTPUT_DIR, \"scanner_fingerprints.pkl\")\n",
    "FP_KEYS_OUT = os.path.join(OUTPUT_DIR, \"fp_keys.npy\")\n",
    "\n",
    "if os.path.exists(os.path.join(OUTPUT_DIR, \"flatfield_residuals.pkl\")):\n",
    "    with open(os.path.join(OUTPUT_DIR, \"flatfield_residuals.pkl\"), \"rb\") as f:\n",
    "        flatfield_res = pickle.load(f)\n",
    "    scanner_fps = {}\n",
    "    for scanner, res_list in tqdm(flatfield_res.items(), desc=\"Computing fingerprints\"):\n",
    "        if not res_list:\n",
    "            continue\n",
    "        stack = np.stack(res_list, axis=0)   # shape (N, H, W)\n",
    "        scanner_fps[scanner] = np.mean(stack, axis=0).astype(np.float32)\n",
    "    save_pickle(scanner_fps, FP_OUT)\n",
    "    np.save(FP_KEYS_OUT, np.array(sorted(scanner_fps.keys())))\n",
    "    print(\"Saved fingerprints:\", FP_OUT, \"keys:\", FP_KEYS_OUT)\n",
    "else:\n",
    "    print(\"flatfield_residuals.pkl not found — run Cell 5 for FLATFIELD_DIR first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ace12abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features (official): 100%|██████████| 11/11 [00:18<00:00,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Infosys_AI-Tracefinder\\Notebooks\\official_features.pkl\n",
      "Feature length: 27 samples: 2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — extract handcrafted features (requires scanner_fps)\n",
    "from skimage.feature import local_binary_pattern\n",
    "from scipy.fft import fft2, fftshift\n",
    "import math\n",
    "\n",
    "def corr2d(a, b):\n",
    "    a = a.astype(np.float32).ravel()\n",
    "    b = b.astype(np.float32).ravel()\n",
    "    a -= a.mean(); b -= b.mean()\n",
    "    denom = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "    return float((a @ b) / denom) if denom > 0 else 0.0\n",
    "\n",
    "def fft_radial_energy(img, K=6):\n",
    "    f = fftshift(fft2(img))\n",
    "    mag = np.abs(f)\n",
    "    h, w = mag.shape\n",
    "    cy, cx = h//2, w//2\n",
    "    yy, xx = np.ogrid[:h, :w]\n",
    "    r = np.sqrt((yy - cy)**2 + (xx - cx)**2)\n",
    "    bins = np.linspace(0, r.max()+1e-6, K+1)\n",
    "    feats = []\n",
    "    for i in range(K):\n",
    "        mask = (r >= bins[i]) & (r < bins[i+1])\n",
    "        sel = mag[mask]\n",
    "        feats.append(float(np.mean(sel)) if sel.size else 0.0)\n",
    "    return feats\n",
    "\n",
    "def lbp_hist_safe(img, P=8, R=1.0):\n",
    "    rng = float(np.ptp(img))\n",
    "    if rng < 1e-12:\n",
    "        return [0.0] * (P + 2)\n",
    "    g = (img - img.min()) / (rng + 1e-8)\n",
    "    g8 = (g * 255).astype(np.uint8)\n",
    "    codes = local_binary_pattern(g8, P=P, R=R, method=\"uniform\")\n",
    "    hist, _ = np.histogram(codes, bins=np.arange(P+3), density=True)\n",
    "    return hist.astype(np.float32).tolist()\n",
    "\n",
    "# require fingerprints\n",
    "if not os.path.exists(FP_OUT):\n",
    "    print(\"Fingerprints (scanner_fingerprints.pkl) missing. Run Cell 6 first.\")\n",
    "else:\n",
    "    with open(FP_OUT, \"rb\") as f:\n",
    "        scanner_fps = pickle.load(f)\n",
    "    fp_keys = np.load(FP_KEYS_OUT, allow_pickle=True).tolist()\n",
    "    features = []\n",
    "    labels = []\n",
    "    # official_residuals must exist (we want features for Official)\n",
    "    if not os.path.exists(OFFICIAL_OUT):\n",
    "        raise FileNotFoundError(\"official_residuals.pkl missing. Run Cell 5 for OFFICIAL_DIR.\")\n",
    "    with open(OFFICIAL_OUT, \"rb\") as f:\n",
    "        official = pickle.load(f)\n",
    "    for scanner, res_list in tqdm(official.items(), desc=\"Extracting features (official)\"):\n",
    "        for res in res_list:\n",
    "            v_corr = [corr2d(res, scanner_fps[k]) for k in fp_keys]\n",
    "            v_fft = fft_radial_energy(res)\n",
    "            v_lbp = lbp_hist_safe(res)\n",
    "            feat_vec = v_corr + v_fft + v_lbp\n",
    "            features.append(feat_vec)\n",
    "            labels.append(scanner)\n",
    "    FEATURES_OUT = os.path.join(OUTPUT_DIR, \"official_features.pkl\")\n",
    "    save_pickle({\"features\": features, \"labels\": labels}, FEATURES_OUT)\n",
    "    print(\"Feature length:\", len(features[0]), \"samples:\", len(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7563d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_img shape: (2200, 256, 256, 1)\n",
      "features shape: (2200, 27)\n",
      "num labels: (2200,)\n",
      "Saved: X_img.npy, X_feat.npy, y.npy and encoder/scaler in D:\\Infosys_AI-Tracefinder\\Notebooks\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — prepare final dataset (X_img.npy, X_feat.npy, y.npy) and save encoders\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "FEATURES_OUT = os.path.join(OUTPUT_DIR, \"official_features.pkl\")\n",
    "if not os.path.exists(FEATURES_OUT):\n",
    "    raise FileNotFoundError(\"official_features.pkl not found — run Cell 7 first.\")\n",
    "\n",
    "with open(FEATURES_OUT, \"rb\") as f:\n",
    "    feat_data = pickle.load(f)\n",
    "features = np.array(feat_data[\"features\"], dtype=np.float32)\n",
    "labels = np.array(feat_data[\"labels\"])\n",
    "\n",
    "# X_img: build from official_residuals (order must match labels)\n",
    "with open(OFFICIAL_OUT, \"rb\") as f:\n",
    "    official = pickle.load(f)\n",
    "\n",
    "X_img = []\n",
    "y_img = []\n",
    "for scanner, res_list in official.items():\n",
    "    for res in res_list:\n",
    "        X_img.append(np.expand_dims(res, -1))   # (H,W,1)\n",
    "        y_img.append(scanner)\n",
    "\n",
    "X_img = np.array(X_img, dtype=np.float32)\n",
    "y_img = np.array(y_img)\n",
    "\n",
    "# Quick check: lengths must match features/labels\n",
    "print(\"X_img shape:\", X_img.shape)\n",
    "print(\"features shape:\", features.shape)\n",
    "print(\"num labels:\", labels.shape)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(labels)\n",
    "\n",
    "# Scale handcrafted features\n",
    "scaler = StandardScaler()\n",
    "X_feat_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Save final artifacts\n",
    "np.save(os.path.join(OUTPUT_DIR, \"X_img.npy\"), X_img)\n",
    "np.save(os.path.join(OUTPUT_DIR, \"X_feat.npy\"), X_feat_scaled)\n",
    "np.save(os.path.join(OUTPUT_DIR, \"y.npy\"), y_encoded)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"hybrid_label_encoder.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "with open(os.path.join(OUTPUT_DIR, \"hybrid_feat_scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Saved: X_img.npy, X_feat.npy, y.npy and encoder/scaler in\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4440854f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "official_residuals.pkl -> FOUND\n",
      "flatfield_residuals.pkl -> FOUND\n",
      "scanner_fingerprints.pkl -> FOUND\n",
      "fp_keys.npy -> FOUND\n",
      "official_features.pkl -> FOUND\n",
      "X_img.npy -> FOUND\n",
      "X_feat.npy -> FOUND\n",
      "y.npy -> FOUND\n",
      "Classes: [np.str_('Canon120-1'), np.str_('Canon120-2'), np.str_('Canon220'), np.str_('Canon9000-1'), np.str_('Canon9000-2'), np.str_('EpsonV370-1'), np.str_('EpsonV370-2'), np.str_('EpsonV39-1'), np.str_('EpsonV39-2'), np.str_('EpsonV550'), np.str_('HP')]\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 — sanity checks\n",
    "for p in [\"official_residuals.pkl\", \"flatfield_residuals.pkl\", \"scanner_fingerprints.pkl\", \n",
    "          \"fp_keys.npy\", \"official_features.pkl\", \"X_img.npy\", \"X_feat.npy\", \"y.npy\"]:\n",
    "    full = os.path.join(OUTPUT_DIR, p)\n",
    "    print(p, \"->\", \"FOUND\" if os.path.exists(full) else \"MISSING\")\n",
    "# show classes\n",
    "import numpy as np\n",
    "with open(os.path.join(OUTPUT_DIR, \"hybrid_label_encoder.pkl\"), \"rb\") as f:\n",
    "    le = pickle.load(f)\n",
    "print(\"Classes:\", list(le.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
